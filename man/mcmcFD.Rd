% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mcmcFD.R
\name{mcmcFD}
\alias{mcmcFD}
\title{First Differences of a Bayesian Logit or Probit model}
\usage{
mcmcFD(modelmatrix, mcmcout, link = "logit", ci = c(0.025, 0.975),
  percentiles = c(0.25, 0.75), fullsims = FALSE)
}
\arguments{
\item{modelmatrix}{model matrix, including intercept (if the intercept is among the
parameters of interest from the model). Create with model.matrix(formula, data).
Note: the order of columns in the model matrix must correspond to the order of columns 
in the matrix of posterior draws in the \code{mcmcout} argument. See the \code{mcmcout}
argument for more}

\item{mcmcout}{posterior distributions of all logit coefficients, 
in matrix form. This can be created from rstan, MCMCpack, R2jags, etc. and transformed
into a matrix using the function as.mcmc() from the coda package for \code{jags} class
objects, as.matrix() from base R for \code{mcmc}, \code{mcmc.list}, \code{stanreg}, and 
\code{stanfit} class objects, and \code{object$sims.matrix} for \code{bugs} class objects.
Note: the order of columns in this matrix must correspond to the order of columns 
in the model matrix. One can do this by examining the posterior distribution matrix and listing the 
variables in the order of this matrix when creating the matrix model. A useful function for sorting as 
you create the matrix of posterior distributions is \code{mixedsort()} fom the gtools package}

\item{link}{type of model. It is a character vector set to \code{"logit"} (default) or \code{"probit"}}

\item{ci}{the bounds of the credible interval. Default is \code{c(0.025, 0.975)}.}

\item{percentiles}{default is \code{c(0.25, 0.75)}}

\item{fullsims}{logical indicator of whether full object (based on all MCMC draws 
rather than average) will be returned. Default is \code{FALSE}}
}
\value{
an object of class \code{matrix} with the first differences for each
covariate
}
\description{
R function to calculate first differences after a Bayesian logit or probit model.
First differences are a method to summarize effects across covariates. This quantity represents
the difference in predicted probabilities for each covariate for cases with low and high values 
of the respective covariate. For each of these differences, all other variables are held constant 
at their median. For more, see Long (1997, Sage Publications) and King, Tomz, and Wittenberg (2000, 
American Journal of Political Science 44(2): 347-361)
}
\examples{
\donttest{
## simulating data
set.seed(123456)
b0 <- 0.2 # true value for the intercept
b1 <- 0.5 # true value for first beta
b2 <- 0.7 # true value for second beta
n <- 500 # sample size
X1 <- runif(n, -1, 1)
X2 <- runif(n, -1, 1)
Z <- b0 + b1 * X1 + b2 * X2
pr <- 1 / (1 + exp(-Z)) # inv logit function
Y <- rbinom(n, 1, pr) 
data <- data.frame(cbind(X1, X2, Y))

## formatting the data for jags
datjags <- as.list(data)
datjags$N <- length(datjags$Y)

## creating jags model
model <- function()  {
  
  for(i in 1:N){
    Y[i] ~ dbern(p[i])  ## Bernoulli distribution of y_i
    logit(p[i]) <- mu[i]    ## Logit link function
    mu[i] <- b[1] + 
      b[2] * X1[i] + 
      b[3] * X2[i]
  }
  
  for(j in 1:3){
    b[j] ~ dnorm(0, 0.001) ## Use a coefficient vector for simplicity
  }
  
}

params <- c("b")
inits1 <- list("b" = rep(0, 3))
inits2 <- list("b" = rep(0, 3))
inits <- list(inits1, inits2)

## fitting the model with R2jags
set.seed(123)
fit <- R2jags::jags(data = datjags, inits = inits, 
                    parameters.to.save = params, n.chains = 2, n.iter = 2000, 
                    n.burnin = 1000, model.file = model)

## running function with logit
xmat <- model.matrix(Y ~ X1 + X2, data = data)
mcmc <- coda::as.mcmc(fit)
mcmc_mat <- as.matrix(mcmc)[, 1:ncol(xmat)]
object <- mcmcFD(modelmatrix = xmat,
                 mcmcout = mcmc_mat)
object
}
}
\references{
\itemize{
\item King, Gary, Michael Tomz, and Jason Wittenberg. 2000. “Making the Most of Statistical 
Analyses: Improving Interpretation and Presentation.” American Journal of Political Science 
44 (2): 347–61. http://www.jstor.org/stable/2669316
\item Kruschke, John K. 2013. “Bayesian Estimation Supersedes the T-Test.” Journal of 
Experimental Psychology: General 142 (2): 573–603. https://doi.org/10.1037/a0029146
\item Long, J. Scott. 1997. Regression Models for Categorial and Limited Dependent Variables. 
Thousand Oaks: Sage Publications
}
}
